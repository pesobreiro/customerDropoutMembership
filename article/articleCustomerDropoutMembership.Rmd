---
title: |
  |
  |
  |
  | \vspace{1cm}Customer dropout membership^[Corresponding address: sobreiro@esdrm.ipsantarem.pt. Quality of Life Research Centre, Polytechnic Institute of Santarém, Portugal]\vspace{0.5cm}
author: |
  | Pedro Sobreiro, Javier Berrocal, Domingos Martinho, José Garcia Alonso
  |
  | Extremadura University
  |
date: |
  |
  |
  | `r gsub("^0", "", format(Sys.time(), "%d %B, %Y"))`
  |
  |
linestretch: 1.2
colorlinks: true
abstract: \noindent\setstretch{1}Abstract of the article. Here we can place more info.\vspace{.8cm}
bibliography: refMembership.bib
csl: american-sociological-association.csl
output:
  bookdown::pdf_document2:
    includes:
    toc: no
    keep_tex: true
    latex_engine: xelatex
    number_sections: true
  bookdown::word_document2:
    latex_engine: xelatex
    number_sections: true
mainfont: Times New Roman
sansfont: Times New Roman
fontsize: 12pt
link-citations: true
documentclass: article
geometry: margin = 1in
always_allow_html: yes
header-includes:
   - \usepackage{dcolumn}
   - \usepackage{color}
   - \usepackage{pdfpages}
   - \usepackage{amsmath}
   - \usepackage{booktabs}
   - \usepackage{makecell}
   - \usepackage{hyperref}
editor_options: 
  markdown: 
    wrap: 80
---

```{r Setup, include = FALSE}
knitr::opts_chunk$set(cache = FALSE)
# Use cache = TRUE if you want to speed up compilation
# set path
# get rmarkdown directory
caminho <- getwd()
# set working directory
setwd(caminho)
print(caminho)

# A function to allow for showing some of the inline code
rinline <- function(code) {
  html <- '<code  class = "r">``` `r CODE` ```</code>'
  sub("CODE", code, html)
}   
```

```{r RETICULATE, message=FALSE, warning=FALSE, include=FALSE}
# load essential libraries
library(dplyr)
library(dlookr)
library(ggplot2)
library(reticulate)
#Replace by your environment, usually which python solves the problem
#conda env list also is a good option
#set environment first then call reticulate library
path_python_windows <- "C:\\Users\\sobre\\AppData\\Local\\r-miniconda\\envs\\rsurvival\\python.exe"
path_python_linux <- "/home/sobreiro/miniconda3/envs/survival/bin/python"
switch(Sys.info()[["sysname"]],
       Windows = {
            Sys.setenv(RETICULATE_PYTHON = path_python_windows)
            #call reticulate
            library(reticulate)
            #activate environment
            use_condaenv("rsurvival", required = TRUE)
       },
       Linux = {
            Sys.setenv(RETICULATE_PYTHON = path_python_linux)
            library(reticulate)
            use_condaenv("survival", required = TRUE)
       }
)
```

# Introduction

Research idea:

Context: An organization membership located in Portugal. The organization offers
an annual membership for the members, the service subscription has several
payment options:

-   Men with a annual fee of 10€
-   Women annual fee of 6€
-   Correspondent fee 6€
-   Retired fee 5€
-   Student fee 2.5€
-   under-14 fee 1€

Churn dropout prediction is a problem being addressed supported in the idea that
the customers database is the most valuable asset that the organizations possess
[@Athanassopoulos_2000], which requires determining customers that will attrite
[@alboukaey_dynamic_2020]. Dropout implies in contractual business that the
customer needs to renew their contracts to continue its usage
[@Ascarza_Hardie_2013].

However, in contractual settings the customer dropout represents an explicit
ending of a relationship more punitive than non contractual settings
[@risselada_staying_2010] This has implications to the profitability of the
organizations increasing marketing costs and reducing sales
[@amin_customer_2017].

The anticipation of the dropout allows the development of countermeasures to
reduce customer churn. Several studies address the problem related to customer
retention trying to improve the profitability [@coussement_improving_2009;
@garcia_intelligent_2017; @devriendt_why_2019]

``` markdown
If an organization can predict a possible dropout and
develop countermeasures to avoid desertion, they can avoid customer defections that 
lead to a loss of money. Reichheld (1996) evidenced that reducing dropout rates by 5% 
(e.g., from 15% to 10% per year) could represent an increase in prots up to double, as 
acquiring new customers costs 5 to 6 times more than retaining existing ones (Bhattacharya, 
1998). Existing organizations are addressing this problem by shifting their target from 
capturing new customers to preserving existing ones (García et al., 2017),
as investments in retention strategies have higher returns than acquisitions (Coussement 
& Van den Poel, 2009). The importance of customer retention to maintain organizational protability (Devriendt
et al., 2019) leads to the problem of how to quantify the nancial impact of customer retention actions
under the assumption that the organization goal should be related to the increase the lifetime of the
customer to increase their prots. The customer lifetime value (CLV) allows us to measure this, as itr
```

1.  Address the global problem of customer dropout
2.  The identification of approaches to predict dropout requires more than only,
    address the prediction accuracy such as ... place existing studies
    addressing this...
3.  A lot of effort has been placed testing the accuracy of existing algorithms,
    in this study we try to fill this gap and explore also a balancing between
    the model interpretability, accuracy, and the investment required.
4.  Dropout prediction problem related to the timings

The approaches normally employing use a dependent variable representing dropout
or non-dropout, without considering a dynamic perspective that the dropout risk
changes over time [@Alboukaey_dynamic_2020]. The survival models try to solve
this limitation [@routh_estimating_2020] capturing a temporal dimension of the
customer dropout [@perianez_churn_2016]. @perianez_churn_2016 used survival
analysis to predict also when the dropout will occur.

Other studies proposed also the integration of several algorithms to improve the
performance in the prediction of the dropout such the usage of clusters combined
with churn prediction [@hung_applying_2006; @gok_case_2015;
@vijaya_sivasankar_2019]. The approach relies in the assumption that combining
the customers in different clusters allows the improvement of the prediction
accuracy. @vijaya_sivasankar_2019 suggested the adoption hybrid models combining
more than one classier are achieving increased performance compared to those
using single classifiers.

There are several challenges around the timing related to dropout, or
considering the dynamic behavior of the customer in the intent to drop out
\[@alboukaey_dynamic_2020\]. The importance of understanding when dropout will
occur and the risk when discarding the temporal perspective of the problem seems
to be an element that should be addressed. Few studies considered this
\[@perianez_churn_2016; @burez_separating_2008\]. This shows an opportunity to
address the importance of the timeframe and its influence on the efficiency of
the model and also evalute if the combination of clusters could improve the
performance.

In this study, we adopt random survival forests which have never been used in
understanding factors affecting membership in a sport club using existing data
in a Sport Club. The analysis is based on the use of random survival forests in
the presence of covariates that do not necessarily satisfy the PH assumption.
Additionly we also propose a new approach combining clusters with survival
analysis.

??? Add interpretability layer

Random Survival Forests does not make the proportional hazards assumption
[@Ehrlinger_2016] and has the flexibility to model survivor curves that are of
dissimilar shapes for contrasting groups of subjects. Random Survival Forest is
an extension of Random Forest allowing efficient non-parametric analysis of time
to event data [@Breiman_2001]. This characteristics allow us to surpass the Cox
Regression limitation of the proportional hazard assumption, requiring to
exclude variables which not fulfill the model assumption. It was shown by
@Breiman_2001 that ensemble learning can be further improved by injecting
randomization into the base learning process - a method called Random Forests.

# Methodology

Dropout is a binary value where one represent churn and zero not churn. The
dropout happens when a member does not have a payment ...

The model performance was determined with the concordance probability (C-index),
Brier Score (BS) and Mean Absolute Error (MAE) [@wangmachine2017]. The feature
importance was determined calculating the difference between the true class
label and noised data [@Breiman_2001].

## Dataset

```{r get_data, echo = FALSE, message = FALSE, warning=FALSE, include = FALSE}
library(stargazer)
library(readxl)
library(dplyr)
library(visdat)

df_members <- read_excel("../data/membershipData.xlsx")

names(df_members)
# rename column labels
names(df_members) <- c("num_socio", "dt_inscription", "year_inscription", "birth_date",
                       "age", "sex", "marital_status", "category",
                       "monthly_fee", "occupation", "zip_code",
                       "dt_last_invoice", "dt_last_payment", "total_amount",
                       "total_matches", "season_matches",
                       "days_since_last_payment", "months_since_last_payment",
                       "dropout", "years_membership", "stadium_access",
                       "quart_stadium_entries", "inscription_month")

names(df_members)

# select relevant variables
df_members <- df_members %>%
select(age, sex, marital_status, monthly_fee, total_amount, total_matches,
       season_matches, months_since_last_payment, dropout, years_membership,
       stadium_access, quart_stadium_entries,inscription_month)

str(df_members)
vis_dat(df_members) #check
```

Table \@ref(tab:summarytable) shows data's summary statistics. The average age
is `r round(mean(df_members$age,na.rm = TRUE),1)` ±
`r round(sd(df_members$age,na.rm = TRUE),1)`, the members have an attendance of
`r round(mean(df_members$total_matches,na.rm = TRUE),0)` ±
`r  round(sd(df_members$total_matches,na.rm = TRUE),1)` with a membership of
`r round(mean(df_members$years_membership,na.rm = TRUE),0)` ±
`r  round(sd(df_members$years_membership,na.rm = TRUE),1)` years.

```{r teste, eval=FALSE, message=FALSE, warning=FALSE, echo=FALSE, paged.print=FALSE}
library(stargazer)
df_summary <- summary(df_members)
stargazer(df_members,
          title = "Summary statistics",
          label = "tab1cars",
          table.placement = "h",
          header = FALSE,
          summary = TRUE,
          summary.stat = c("min", "p25", "median", "p75", "max", "median", "sd")
         )
```

```{r summarytable, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
library(gtsummary)
library(kableExtra)
library(labelled)

var_label(df_members$age) <- "Age in years"
var_label(df_members$sex) <- "Male or female"
var_label(df_members$marital_status) = "Single, married and other."

tbl <- df_members %>%
    tbl_summary(statistic = list(all_continuous() ~ "{mean} ({sd})",
                                 all_categorical() ~ "{p}%"),
                type = list(age ~ "continuous")) %>%
    add_stat_label()

as_kable_extra(tbl, booktabs = T,
               caption = "Summary statistics of features used")
```

Figure \@ref(fig:membershipyear) shows the distribution of the dropout
considering the number of years of membership.

```{r membershipyear, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Number of members by year"}
members_year <- df_members %>%
  select(years_membership, dropout) %>%
  group_by(years_membership, dropout) %>%
  summarize(count = n())

members_year$dropout <- factor(members_year$dropout)
g <- ggplot(data = members_year,
            mapping = aes(x = years_membership, y = count,linetype = dropout))
g + geom_line() + labs(x = "year", y = "Number of members") +
    theme_classic()
```

## Model construction

Address the model construction... the categorical variables $sex$,
$marital\_status$ and $quart\_stadium\_entries$ where converted to dummy
variables.

The random survival forest was developed using the package PySurvival
[@Fotso_others_2019]. The most relevant variables predicting the dropout are
analysed using the log-rank test. The metric variables are transformed to
categorical using the quartiles to provide a statistical comparison of groups.
The survival analysis was conducted using the package Lifelines
[@Davidson-Pilon_2021].

PySurvival is an open source python package for Survival Analysis modeling - the
modeling concept used to analyze or predict when an event is likely to happen.
It is built upon the most commonly used machine learning packages such NumPy,
SciPy and PyTorch. PySurvival is compatible with Python 2.7-3.7

### Survival trees based model

In this model... The survival trees based model uses pysurvival random forest

```{python Py,eval=TRUE, echo=FALSE}
from pysurvival.utils.display import correlation_matrix
import pandas as pd
import numpy as np

col = ['sex','marital_status','quart_stadium_entries']

df_members = r.df_members #copy r dataframe to python

df_members = pd.get_dummies(df_members, columns=col,drop_first=True)

# Creating the time and event columns
time_column = 'years_membership'
event_column = 'dropout'

# Extracting the features
features = np.setdiff1d(df_members.columns, [time_column, event_column] ).tolist()


correlation_matrix(df_members[features], figure_size=(10,10), text_fontsize=6)

r.df_members = df_members
```

Removed the variables with greater correlations $total\_matches$ and
$quart\_stadium\_entries$

```{python removeVar, eval=TRUE, echo=FALSE, warning=FALSE}
to_remove = ['total_matches', 'quart_stadium_entries_ate 1']

#consider also the features previously removed
features = np.setdiff1d(df_members[features].columns, to_remove).tolist()

df_members.drop(columns = to_remove, inplace=True)

correlation_matrix(df_members[features], figure_size=(10,10), text_fontsize=6)
```

```{python createModel, eval=TRUE, echo=FALSE, cache = FALSE, warning = FALSE}
from pysurvival.models.survival_forest import RandomSurvivalForestModel
from sklearn.model_selection import train_test_split
from pysurvival.utils.metrics import concordance_index
from pysurvival.utils.display import integrated_brier_score
from pysurvival.utils.display import compare_to_actual

X = df_members.copy()
t = df_members['years_membership']
e = df_members['dropout']
X.drop(axis=1, columns=['years_membership','dropout'])

X_train, X_test, t_train, t_test, e_train, e_test = train_test_split(X, t, e, test_size=0.3, random_state=0)

# Fitting the model
csf = RandomSurvivalForestModel(num_trees=20)
csf.fit(X_train, t_train, e_train, max_features='sqrt', max_depth=5, min_node_size=20, seed = 1)

c_index = concordance_index(csf, X_test, t_test, e_test)
ibs = integrated_brier_score(csf, X_test, t_test, e_test, t_max=12, figure_size=(12,5))
results = compare_to_actual(csf, X_test, t_test, e_test, is_at_risk = False,  figure_size=(12, 6), metrics = ['rmse', 'mean', 'median'])

print(results)
```

Table \@ref(tab:summarytable2) shows variables importance.

```{r summarytable2, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
tbl <- py$csf$variable_importance_table
kbl(tbl, booktabs = T, caption = "Summary statistics of features used")
```

#### Model building

The model was built with with 70% of the data for training and 30% for testing.
The survival model parameters where:

The model accuracy is very high in the first years. The prediction is very
similar to the actual value. The absolute error mean of
`r round(py$results$mean_absolute_error,0)` customers.

### Survival trees based model with clusters

Here we are will create clusters and developed the optimization within each
cluster...

The calculation of he number of clusters used the package mclust [@scrucca2016]
using the Bayesian Information Criterion (BIC). The model that gives the minimum
BIC score can be selected as the best model [@schwarz1978] simplifying the
problem related to choosing the number of components and identifying the
structure of the covariance matrix, based on modelling with multivariate normal
distributions for each component that forms the data set [@akogul2016].

In multivariate models are available the following approaches:

-   "EII"spherical, equal volume
-   "EEE"ellipsoidal, equal volume, shape, and orientation
-   "VII"spherical, unequal volume
-   "VVV"ellipsoidal, varying volume, shape, and orientation, which is used as
    default for initialization of EM algorithm
-   VVI": diagonal, varying volume and shape t

Estou a ter problemas com o cálculo dos clusters com o BIC... estava a aqui a
confirmar os clusters, talvez seja melhor reduzir as variáveis... testar a
abordagem aos clusters no artigo dos vinhos...

```{r clusters, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(mclust)
y <- scale(py$df_members)
bic <- mclustBIC(y,modelNames = c("VEI"))
plot(bic)

temp = data.frame(bic[,'VEI'])
temp$cluster = row.names(temp)

names(temp) = c("val","cluster")

kbl(temp, booktabs = T, caption = "Summary statistics of features used")

mod1 <- Mclust(py$df_members,x=BIC)
summary(mod1)

#plot(mod1, what="uncertainty")
```

```{r NbClust, eval=FALSE}
library(NbClust)
nb <- NbClust(y, diss=NULL, distance = "euclidean", 
              min.nc=2, max.nc=5, method = "kmeans", 
              index = "all", alphaBeale = 0.1)
hist(nb$Best.nc[1,], breaks = max(na.omit(nb$Best.nc[1,])))

```

```{python elbowCalculation, eval=TRUE, echo=FALSE, cache=FALSE, warning = FALSE}
from sklearn.cluster import KMeans
from sklearn import preprocessing
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt

import numpy as np
#Finding optimal no. of clusters
clusters=range(1,20)
meanDistortions=[]
 
for k in clusters:
    model=KMeans(n_clusters=k)
    model.fit(df_members)
    prediction=model.predict(df_members)
    meanDistortions.append(sum(np.min(cdist(df_members, model.cluster_centers_, 'euclidean'), axis=1)) / df_members.shape[0])
 
plt.plot(clusters, meanDistortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Average distortion')
plt.show();

```

We are going to consider five clusters

```{python kmeans calculation, eval=TRUE, echo=FALSE, cache=FALSE, warning = FALSE}
cluster = KMeans(n_clusters=5)

cluster.fit(df_members)
df_members['cluster']=cluster.predict(df_members)
print(df_members.cluster.value_counts());
```

```{python kmeans PCA, eval=TRUE, echo=FALSE, cache=FALSE, warning = FALSE}
from sklearn.decomposition import PCA

plt.rcParams['figure.figsize'] = [13, 4]

pca=PCA(n_components=2)
df_members['X_pca']=pca.fit_transform(df_members)[:,0] #todas as linhas da primeira coluna com redução
df_members['Y_pca']=pca.fit_transform(df_members)[:,1] #todas as linhas da segunda coluna com redução

fig, ax = plt.subplots()
for cluster in df_members.cluster.unique():
    x = df_members['X_pca'].loc[df_members.cluster == cluster]
    y = df_members['Y_pca'].loc[df_members.cluster == cluster]
    ax.scatter(x, y, label=cluster, alpha=1, edgecolors='none');

ax.legend()
ax.grid(True)
ax.set_title('Clusters clientes');
```

TODO: Estava aqui... fit the model for the five clusters... compare the
performance with the model without clusters... corrigir acima para selecionar só
as features.. Explorar t-SNE for better visualization...

```{python createModelClusters, eval=TRUE, echo=FALSE, cache = TRUE, warning = FALSE}
# Building training and testing sets for the clusters
for cluster in df_members.cluster.unique():
    # Number of samples in the dataset
    df_members_cluster = df_members[df_members.cluster == cluster]
    X = df_members_cluster.copy()
    t = df_members_cluster['years_membership']
    e = df_members_cluster['dropout']
    X.drop(axis=1, columns=['years_membership','dropout'])
    X_train, X_test, t_train, t_test, e_train, e_test = train_test_split(X, t, e, random_state=0)
    print(f"The cluster {cluster} as a size of {X.shape[0]}")
    # Fitting the model
    csf = RandomSurvivalForestModel(num_trees=20)
    csf.fit(X_train, t_train, e_train, max_features='sqrt',
        max_depth=5, min_node_size=20, seed = 1)

    c_index = concordance_index(csf, X_test, t_test, e_test)

    ibs = integrated_brier_score(csf, X_test, t_test, e_test, t_max=12, figure_size=(12,5))

    results = compare_to_actual(csf, X_test, t_test, e_test, is_at_risk = False,  
                                figure_size=(12, 6), metrics = ['rmse', 'mean', 'median'])
    print(f"Cluster {cluster} RMSE {results['root_mean_squared_error']}")
    print(f"Cluster {cluster} MAE {results['median_absolute_error']}")
    print(f"Cluster {cluster} MAError {results['mean_absolute_error']}")
```

# Open questions - to remove

```{=tex}
\begin{itemize}
    \item RQ1: What is the current state of the research being developed?
    \item RQ2: What algorithms have been used to predict dropout?
    \item RQ3: What are the features used to predict dropout?
    \item RQ4: When does dropout occur?
    \item RQ5: How is the accuracy of the machine learning algorithms in predicting dropout measured?
\end{itemize}
```
From RQ1, it was possible to identify some business areas that are
under-researched, such as the energy sector, education, logistics and
hospitality. Compared to other business areas such telecom or the financial
sector, research on the energy sector or water supply is lacking, considering
the contractual settings that are assumed to provide such types of services.
Considering the business model of many software companies as software as a
services (SaS), the number of research works is also surprisingly low.

RQ2 also provided an overall perspective related to the algorithms being used to
predict customer dropout. The first could be the importance and wider adoption
of decision trees and random forests [@antipov_applying_2010;
@benoit_improving_2012; @burez_crm_2007], and logistic regression
[@coussement_improved_2010], which could be due to its higher interpretability
and flexibility [@keramati_improved_2014]. Interpretability is an important
aspect for the marketing department in the extraction of valuable information
from the model to develop effective retention strategies [@verbeke_new_2012].
The problem arises in the balancing between interpretability and the higher
performance of the algorithms inspired by nature (such as neural networks). From
a business perspective, dropout prediction should also be considered as a
business objective, which requires more than predicting if the customer will
churn or not [@devriendt_why_2019], where higher interpretability provides
better support in the development of retention strategies. The developed SLR
also raises the possibility of integrating different algorithms using ensemble
methods or integrating several models using a hybrid approach. None of the
studies integrated the survival approach to predict customer dropout, for
example, using a hybrid approach.

It is considered positive if actions are developed to retain customers, but the
problems should also be considered, such as the following: (1) customers who
have greater risk of dropout should be targeted to provide a base for a better
ROI in the retention strategies [@xie_customer_2009; @coussement_churn_2008] and
(2) the retention strategies should be developed focused on customers with
higher satisfaction, or its inclusion could be a reminder of the contractual
agreement nearing an end and could lead to churn [@devriendt_why_2019].

From RQ3, several types of features being used were able to be identified, such
as demographic, behavioral, and economic indicators, pictorial data, network
relationships or high cardinality features. The problem that arises is that some
studies used data and features that were not described, and this creates a major
issue, How can reproducibility be developed in a study without the availability
of the data or the identification of the features used? Considering that science
is driven by data, with the development of new technologies, the increasing
complexity of research and the amount of data collected, the challenge is to
ensure that research is available to all [@Hanson_Sugden_Alberts_2011]; this
requires both availability of the data and the algorithms so that they can be
explored by other researchers. The features are selected mainly to verify the
performance of the models, and are essential to performance prediction,
accuracy, and the steps for processing the data, which are fundamental to
improve the model accuracy [@azeem_churn_2017].

There are several challenges around the timing related to dropout, or
considering the dynamic behavior of the customer in the intent to drop out
\[@alboukaey_dynamic_2020\]. The importance of understanding when dropout will
occur and the risk when discarding the temporal perspective of the problem seems
to be an element that should be addressed. Few studies considered this
\[@perianez_churn_2016; @burez_separating_2008\]. This shows an opportunity to
address the importance of the timeframe and its influence on the efficiency of
the model.

According to each business model, the timeframe could be addressed considering
the survival probability according to the customer relationship age, and dropout
predictions could be developed according to these survival probabilities, as
suggested by @esteves_churn_2016, to investigate which data timeframe produces
the best result and how the efficiency of the models is influenced by this
timeframe. Exploring the duration of the relation and the understanding of the
features that increase or decrease that duration seems to be an important
approach that could complement the existing approaches to predicting dropout.

From RQ5, the literature analysis showed that different types of questions
arise. Which are the best approaches to develop the analysis of the performance
in predicting dropout? Several metrics are customer dropout is to improve the
performance of organizations in retaining customers, which is a management
problem in which data mining is adopted \[@verbeke_new_2012\]. The goals of the
model should be formulated considering the context of the problem that is being
addressed; in marketing retention strategies, the up-lift supports the
development of proactive actions to minimize the investment in retention
strategies \[@coussement_churn_2008\]. Some assumptions that underlie the
adoption of uplift metrics consider that customers with a higher risk of
churning could not be the best targets, as suggested by @ascarza_retention_2018.
Other researchers addressed the problem using the top-decile lift to develop
more proactive actions to retain the customers at risk of churning
\[@coussement_churn_2008;xie_customer_2009\]. This approach considers the 10% of
customers with more risk, and investments in retention strategies should be
developed that distinguish the churners susceptible to marketing actions from
those who will leave anyway [@coussement_comparative_2017]. Although uplift
models seem to be good strategies, they should also used, such as AUC,
sensitivity, specificity, recall, precision, and F-score. However, the goal
ofconsider factors other than risk and customer satisfaction, as not taking this
into consideration could be counterproductive and the model should be removed
from the retention strategy.

The true business objective is to reduce customer churn. Customers who are about
to churn but cannot be retained should be excluded from the campaign, as
targeting them will be a waste of scarce resources [@devriendt_why_2019]. Using
these models seem to be a good strategy, as they can outperform predictive
models that consider only accuracy from a profitability busshould be considered
that customers with a higher risk of churning may not be the best targets to
develop retention strategies. Those perspectives entail the dropout.

that a business context, or the clarification of a business objective underlying
the prediction of customer dropout, should be developed, to clarify which
objectives should be achieved before employing the profitability of reducing g
machine learning algorithms. Surprisingly, the analyzed studies did not address
the customer lifetime value as an objective to optimize considerininess
perspective.

# Aspects to consider

-   Interpretability from RQ2
-   The business objective is to increase the number of members and organization
    profits
-   piping several algorithms to improve accuracy. Aka hybrid approach
-   @Alboukaey_dynamic_2020 proposes ...
-   grep the articles addressing hybrid: pdfgrep -ri "hybrid.{1,10} approach"

# Results

In this section, we present our experiments to validate the proposed models,
comparing against other approaches. The models where optimized using the
hyper-parameters Grid Search technique. The explored hyper-parameters and the
best values of these parameters for every model are listed in (Table 3).

```{=latex}
\begin{table}[ht]
    \footnotesize
    \centering
    \begin{tabular}{p{0.15\textwidth}p{0.55\textwidth}p{0.25\textwidth}}
    \hline
    \textbf{Model name} & \textbf{Explored parameters values} & \textbf{Best parameters} \\
    \hline
    Survival trees  & pysurvival random forest                  & a \\
    Survival trees
    with clusters   & pysurvival random forest with clusters    & a \\
    Scikit survival 
    trees           & scikit survival                           & a \\
    Scikit survival 
    with clusters   & scikit with clusters                      & a \\
    Scikit survival 
    gradient boost  & scikit survival gradient boost            & a \\
    Scikit survival 
    gradient boost 
    with clusters   & scikit with clusters                      & a \\
    \hline
    \end{tabular}
    \caption{Hyper-parameters best values}
    \label{hyperparametersbestvalues}
\end{table}
```
The results of the performance of the models are available in table 4. Colocar o
modelo. Resultados do modelo

```{=latex}
\begin{table}[ht]
    \footnotesize
    \centering
    \begin{tabular}{p{0.15\textwidth}p{0.35\textwidth}r p{0.10\textwidth}}
    \hline
    \textbf{Model name} & \textbf{Results} & \textbf{n}\\
    \hline
    Survival trees  & RMSE 57.815                                     & 25316\\
                    & MAE 18.966                                      & \\   
                    & MEAE 38.557                                     & \\
    Survival trees  & Cluster 0: RMSE  2.785 MAE  0.000 MEAE  0.899   &   930\\
    with clusters   & Cluster 1: RMSE 78.141 MAE 28.537 MEAE 56.989   & 17067\\
                    & Cluster 2: RMSE 15.081 MAE  1.995 MEAE  6.850   &  2423\\
                    & Cluster 3: RMSE  9.354 MAE  2.506 MEAE  6.427   &  2816\\
                    & Cluster 4: RMSE  8.805 MAE  1.349 MEAE  3.725   &  2080\\
                    
    Scikit survival 
    trees           & scikit survival                                 & \\
    Scikit survival 
    with clusters   & scikit with clusters                            & \\
    Scikit survival 
    gradient boost  & scikit survival gradient boost                  & \\
    Scikit survival 
    gradient boost 
    with clusters   & scikit with clusters                            & \\
    \hline
    \end{tabular}
    \caption{Hyper-parameters best values}
    \label{hyperparametersbestvalues}
    * Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Median Absolute Error (MEAE)  
\end{table}
```
# Conclusion

Article Ascarza

-   Retention Futility: Targeting High-Risk Customers Might be Ineffective
    [@ascarza_retention_2018]

Ascarza, E. (2018). Retention Futility: Targeting High-Risk Customers Might be
Ineffective. Journal of Marketing Research, 55(1), 80-98. sim.
<https://doi.org/10.1509/jmr.16.0163>

Example of Developed actions place in the discussion:

``` markdown
Each month, the company identified the customers who were up for renewal and
split them (randomly and evenly) between a treatment group that received a 
"thank you" gift with the letter and a control group that received only the 
renewal latter.
```

# References {.unnumbered}

::: {#refs}
:::

# Appendix: Chunk options {.unnumbered}

## Software versioning

### R

```{r fig-versioning, echo = TRUE}
cat(paste("#", capture.output(sessionInfo()), "\n", collapse  = ""))
  # or use message() instead of cat()
```

### Other used tools

-   [Visidata](https://www.visidata.org/) for quick exploratory. VisiData is a
    free, open-source tool that lets you quickly open, explore, summarize, and
    analyze datasets in your computer's terminal.
